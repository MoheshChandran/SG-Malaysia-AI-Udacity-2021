# Activation Functions

Neural networks can be used to make decisions on whether or not to transmit signals to other layers of the network. Activation functions are functions that we can use as a decision boundary to inform us when to transmit a signal to subsequent layers in the network. These functions allow us to adjust the likelihood of a certain decision based on inputs, such as a weight for each input, and a decision threshold. To illustrate this basic threshold, a basic threshold is in essence a step function. This means that until we pass our threshold we do not pass a signal, i.e we have a zero or no decision, and once we reached the threshold, we immediately jumped to one.

When executing a complex task such as transcribing a document, we want to allow for some uncertainty. This means that we need a better decision boundary that will give us room for error. Using a sigmoid function we receive an output with a range of zero to one. And this provides a continuous decision boundary where each node in the network acts as a sort of knob.

In machine learning, we actually have many different types of nodes, that each of which has their own pros and cons, and are used for various applications. Given an input, you will provide the machine with the desired output, via your training labels in the training data. The machine will then incrementally update those weight parameters, to best optimize the model for all input output pairs. This is how the model learns from the trainingData, and why the training Data is absolutely critical to the model creation process. The actual method for updating the weights is called Back Propagation, and it is an algorithm used to automatically update those weights on the inputs in each node. It is commonly used in what's commonly known as a Multi-layer Perceptron which is a specific type of neural network. for various
